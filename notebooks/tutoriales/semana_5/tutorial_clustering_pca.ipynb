{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este notebook vamos ver cómo aplicar los diferentes algoritmos de clustering a la data de iris y algunas aplicaciones de la PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta dataset se podía cargar directamente de scikit, dentro del módulo datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargo los datos\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino la matriz de features\n",
    "X = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino el vector de targets\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Miro la distribución de las clases para cada variable mediante histogramas\n",
    "\n",
    "fig, axs= plt.subplots(nrows=2, ncols=2)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    \n",
    "    axs[i].hist(X[y==0,i])\n",
    "    axs[i].hist(X[y==1,i])\n",
    "    axs[i].hist(X[y==2,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También usando scatter plots\n",
    "fig, axs= plt.subplots(nrows=3, ncols=2)\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].scatter(X[:,0],X[:,1], c = y)\n",
    "axs[1].scatter(X[:,0],X[:,2], c = y)\n",
    "axs[2].scatter(X[:,0],X[:,3], c = y)\n",
    "axs[3].scatter(X[:,1],X[:,2], c = y)\n",
    "axs[4].scatter(X[:,1],X[:,3], c = y)\n",
    "axs[5].scatter(X[:,2],X[:,3], c = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a estandarizar los datos, puesto que lo vamos a necesitar para hacer el kmeans (dado que todas las variables tienen que ser del mismo rango) y para hacer la pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame( ss.fit_transform(X), columns = data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos diferentes algoritmos de clustering y vemos su rendimiento en la reconstrucción de los tres grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "nclusters = 3\n",
    "\n",
    "km = KMeans(n_clusters=nclusters, random_state=0, n_init=100)\n",
    "km.fit(X_scaled)\n",
    "\n",
    "# predict the cluster for each data point\n",
    "y_cluster_kmeans = km.predict(X_scaled)\n",
    "y_cluster_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=nclusters, n_init = 100, random_state=100)\n",
    "gmm.fit(X_scaled)\n",
    "\n",
    "# predict the cluster for each data point\n",
    "y_cluster_gmm = gmm.predict(X_scaled)\n",
    "y_cluster_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3,compute_full_tree=True)\n",
    "y_cluster_agg = agg.fit_predict(X_scaled)\n",
    "y_cluster_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score\n",
    "print(completeness_score(y, y_cluster_kmeans))\n",
    "print(completeness_score(y, y_cluster_gmm))\n",
    "print(completeness_score(y, y_cluster_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "print(adjusted_mutual_info_score(y, y_cluster_kmeans))\n",
    "print(adjusted_mutual_info_score(y, y_cluster_gmm))\n",
    "print(adjusted_mutual_info_score(y, y_cluster_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el número de clusters usando el método elbow. En este caso, podemos llamar al atributo *inertia_* de kmeans después de fittearlo. Dicho atributo nos devuelve la suma del cuadrado de las distancias a los centroides, que es lo que optimiza kmeans. Para más info, podéis ver la documentación de kmeans en scikit http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_scores = []\n",
    "for k in np.arange(1,20):\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=100)\n",
    "    km.fit(X_scaled)\n",
    "    kmeans_scores.append(km.inertia_)\n",
    "\n",
    "plt.plot(np.arange(1,20), kmeans_scores)\n",
    "plt.xticks(np.arange(1,20))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA para la visualización\n",
    "\n",
    "Vamos a ver cómo usar la pca, en este caso para facilitarnos la visualización de las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data = X_pca\n",
    "             , columns = ['pc1', 'pc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_df.iloc[y==0,0].values,pca_df.iloc[y==0,1].values)\n",
    "plt.scatter(pca_df.iloc[y==1,0].values,pca_df.iloc[y==1,1].values)\n",
    "plt.scatter(pca_df.iloc[y==2,0].values,pca_df.iloc[y==2,1].values)\n",
    "plt.legend(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez fitteada la pca, podemos ver el ratio de la variación explicada por cada componente accediendo al atributo *explained_variance_ratio_* del objeto de la pca. Para más info, podéis acceder a la documentación de la clase PCA en scikit http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"la variacion de cada compenentes es = \" , pca.explained_variance_ratio_)\n",
    "print( \"la variacion total explicada por las componentes de la pca es = \" , np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([1,2], pca.explained_variance_ratio_)\n",
    "plt.xticks([1,2])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podríamos haberle pedido que nos hiciera la descomposición en tres componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data = X_pca\n",
    "             , columns = ['pc1', 'pc2','pc3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"la variacion total explicada en este caso es = \" , np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pca_df.iloc[y==0,0].values,pca_df.iloc[y==0,1].values, pca_df.iloc[y==0,2].values)\n",
    "ax.scatter(pca_df.iloc[y==1,0].values,pca_df.iloc[y==1,1].values, pca_df.iloc[y==0,2].values)\n",
    "ax.scatter(pca_df.iloc[y==2,0].values,pca_df.iloc[y==2,1].values,pca_df.iloc[y==0,2].values)\n",
    "ax.legend(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, cuando usamos la pca, también podemos pasarle la variación de los datos que queremos que explique. Se lo pasamos como un float entre 0 y 1 al argumento *n_components*. De esta manera, la pca coge las componentes que calculan la variación de los datos que queremos explicar. Una vez ajustada la pca a los datos, podemos acceder al atributo *n_components_* (con una sola barra baja al final) para saber las componentes seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por ejemplo pedimos al menos el 70%\n",
    "pca = PCA(n_components=0.70)\n",
    "pca.fit(X_scaled)\n",
    "print( \"El numero de componentes seleccionadas es = \" , pca.n_components_)\n",
    "print( \"la variacion total explicada en este caso es = \" , np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA para reducir el tiempo computacional (Opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar pca para reducir el número de dimensiones y así reducir la carga computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, \n",
    "                                                    mnist.target, \n",
    "                                                    test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"el tiempo que ha tardado usando toda la data:\", end_time - start_time)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf.fit(X_train_pca, y_train)\n",
    "end_time = time.time()\n",
    "print(\"el tiempo que ha tardado usando el 95%:\", end_time - start_time)\n",
    "print(clf.score(X_test_pca, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
