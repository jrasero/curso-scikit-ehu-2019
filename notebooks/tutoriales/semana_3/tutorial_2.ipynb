{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En este notebook vamos a aplicar algunas herrammientas aprendidas para hacer clasificación. Para ello utilizaremos la Mammographic Dataset, que se encuentra en http://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "1. Title: Mammographic Mass Data\n",
    "\n",
    "2. Sources:\n",
    "\n",
    "   (a) Original owners of database:\n",
    "        Prof. Dr. R�diger Schulz-Wendtland\n",
    "        Institute of Radiology, Gynaecological Radiology, University Erlangen-Nuremberg\n",
    "        Universit�tsstra�e 21-23\n",
    "        91054 Erlangen, Germany\n",
    "        \n",
    "   (b) Donor of database:\n",
    "        Matthias Elter\n",
    "        Fraunhofer Institute for Integrated Circuits (IIS)\n",
    "        Image Processing and Medical Engineering Department (BMT) \n",
    "        Am Wolfsmantel 33\n",
    "        91058 Erlangen, Germany\n",
    "        matthias.elter@iis.fraunhofer.de\n",
    "        (49) 9131-7767327 \n",
    "        \n",
    "   (c) Date received: October 2007\n",
    " \n",
    "3. Past Usage:\n",
    "    M. Elter, R. Schulz-Wendtland and T. Wittenberg (2007)\n",
    "    The prediction of breast cancer biopsy outcomes using two CAD approaches that both emphasize an intelligible decision process.\n",
    "    Medical Physics 34(11), pp. 4164-4172\n",
    "\n",
    "4. Relevant Information:\n",
    "    Mammography is the most effective method for breast cancer screening\n",
    "    available today. However, the low positive predictive value of breast\n",
    "    biopsy resulting from mammogram interpretation leads to approximately\n",
    "    70% unnecessary biopsies with benign outcomes. To reduce the high\n",
    "    number of unnecessary breast biopsies, several computer-aided diagnosis\n",
    "    (CAD) systems have been proposed in the last years.These systems\n",
    "    help physicians in their decision to perform a breast biopsy on a suspicious\n",
    "    lesion seen in a mammogram or to perform a short term follow-up\n",
    "    examination instead.\n",
    "    This data set can be used to predict the severity (benign or malignant)\n",
    "    of a mammographic mass lesion from BI-RADS attributes and the patient's age.\n",
    "    It contains a BI-RADS assessment, the patient's age and three BI-RADS attributes\n",
    "    together with the ground truth (the severity field) for 516 benign and\n",
    "    445 malignant masses that have been identified on full field digital mammograms\n",
    "    collected at the Institute of Radiology of the\n",
    "    University Erlangen-Nuremberg between 2003 and 2006.\n",
    "    Each instance has an associated BI-RADS assessment ranging from 1 (definitely benign)\n",
    "    to 5 (highly suggestive of malignancy) assigned in a double-review process by\n",
    "    physicians. Assuming that all cases with BI-RADS assessments greater or equal\n",
    "    a given value (varying from 1 to 5), are malignant and the other cases benign,\n",
    "    sensitivities and associated specificities can be calculated. These can be an\n",
    "    indication of how well a CAD system performs compared to the radiologists.\n",
    "\n",
    "5. Number of Instances: 961\n",
    "\n",
    "6. Number of Attributes: 6 (1 goal field, 1 non-predictive, 4 predictive attributes)\n",
    "\n",
    "7. Attribute Information:\n",
    "   1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "   2. Age: patient's age in years (integer)\n",
    "   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "   6. Severity: benign=0 or malignant=1 (binominal)\n",
    "\n",
    "8. Missing Attribute Values: Yes\n",
    "    - BI-RADS assessment:    2\n",
    "    - Age:                   5\n",
    "    - Shape:                31\n",
    "    - Margin:               48\n",
    "    - Density:              76\n",
    "    - Severity:              0\n",
    "\n",
    "9. Class Distribution: benign: 516; malignant: 445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerías esenciales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\"\n",
    "mam_data=pd.read_csv(data_url, na_values='?', header=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos info de los datos\n",
    "mam_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y su estadística más descriptiva\n",
    "mam_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las primeras líneas\n",
    "mam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinamos el nombre de las columnas, modificando el atributo columns\n",
    "mam_data.columns=['bi_rads', 'age','shape','margin','density','severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteemos los datos en histogramas, por ejemplo, para ver las \n",
    "# diferencias entre grupos con diferente severidad\n",
    "fig, axs=plt.subplots(ncols=2, nrows=3)\n",
    "axs=axs.flatten()\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.hist(mam_data.dropna()[mam_data.dropna().iloc[:,5]==0].iloc[:,i])\n",
    "    ax.hist(mam_data.dropna()[mam_data.dropna().iloc[:,5]==1].iloc[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotear categorical y ordinal data usando seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una librería muy conocida en python llamada seaborn, que se basa en matplotlib pero que a diferencia de ésta, trabaja nativamente con dataframes, permitiendo hacer plots más avanzados en menos líneas. No es el objetivo de este curso saber usar esta librería. Quien esté interesado, puede visitar la página oficial https://seaborn.pydata.org/ y mirar diferentes tutoriales que se pueden encontrar por internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for var in ['bi_rads','shape','margin','density']:\n",
    "    plt.figure()\n",
    "    sns.countplot(x=var, hue='severity', data=mam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='severity', y='age', data=mam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin NANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos podido comprobar, la dataset contienen NaNs. En scikit, introducir una matriz de features o vector de labels con NaNs daría error. Por ello, tenemos que tratarlos de alguna forma. El método más sencillo sería desechar aquellas observaciones (filas de la matriz de features) que contengan NaNs. Esto se puede hacer fácilmente mediante el método **dropna** de los dataframes de pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mam_data_clean = mam_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mam_data_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos como siempre una matriz de features y un vector de targets o labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mam_data_clean.drop(columns=['bi_rads','severity']).values\n",
    "y=mam_data_clean['severity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos los datos en train (70%) y test (30%) usando la funcion `train_test_split de scikit`, que se encuentra en el módulo `model_selection` (Veremos con detenimiento dicho módulo la semana que viene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos variables categóricas que tenemos que codificar y tratar correctamente. Ya hemos visto cómo hacer esto en scikit mediante el módulo `preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oHe=OneHotEncoder(categorical_features=[1,2], sparse=False)\n",
    "X_train_ohe = oHe.fit_transform(X_train) \n",
    "X_test_ohe = oHe.transform(X_test) # Notad aquí que uso transform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirad la diferencia en el número de columnas de los datos \n",
    "# transformados y sin transformar\n",
    "\n",
    "print(\"el numero de columnas originales = \", X_train.shape[1])\n",
    "print(\"el numero de columnas despues de transformar los datos = \", \n",
    "      X_train_ohe.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar un árbol de decisión y random forest, que no requieren mucho preprocessing de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train_ohe, y_train)\n",
    "print(\"la accuracy del arbol de decision es: \", \n",
    "      clf.score(X_test_ohe, y_test))\n",
    "\n",
    "clf=RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train_ohe, y_train)\n",
    "print(\"la accuracy del random forest es: \", \n",
    "      clf.score(X_test_ohe, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in np.arange(1,10):\n",
    "    clf=DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    clf.fit(X_train_ohe, y_train)\n",
    "    plt.plot(depth, clf.score(X_train_ohe, y_train), '.b')    \n",
    "    plt.plot(depth, clf.score(X_test_ohe, y_test), '.r')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usemos ahora otro clasificador, por ejemplo Support Vector Machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf=SVC(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo requiere que las variables tengan la misma escala. Usemos por ejemplo la clase `MinMaxScaler` para llevar a acabo esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_ohe_scaler =  scaler.fit_transform(X_train_ohe)\n",
    "X_test_ohe_scaler =  scaler.transform(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_ohe.max(axis=0))\n",
    "print(X_train_ohe_scaler.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_ohe_scaler, y_train)\n",
    "print(\"la accuracy de SVM es: \", clf.score(X_test_ohe_scaler, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver cómo varía la clasificación cambiado los parámetros del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in 10**np.arange(-2,2, 0.2):\n",
    "    clf=SVC(C=C, random_state=0)\n",
    "    clf.fit(X_train_ohe_scaler, y_train)\n",
    "    plt.plot(C, clf.score(X_test_ohe_scaler, y_test), '.r') # Mejor en escala logarítmica\n",
    "    #plt.semilogx(C, clf.score(X_test_ohe_scaler, y_test), '.r')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mismo para KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for neigh in np.arange(1,20, 1):\n",
    "    clf=KNeighborsClassifier(n_neighbors=neigh)\n",
    "    clf.fit(X_train_ohe_scaler, y_train)\n",
    "    plt.plot(neigh, clf.score(X_test_ohe_scaler, y_test), '.r')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra medida importante del rendimiento de un clasificador suele ser el área bajo la curva ROC, que te dice cómo varía el clasificador cuando se cambia el treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos las clases que vamos a usar\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fiteamos por ejemplo un support vector machines\n",
    "clf=SVC(random_state=0, probability=True)\n",
    "clf.fit(X_train_ohe_scaler, y_train)\n",
    "print(clf.score(X_test_ohe_scaler, y_test))\n",
    "y_pred_probs = clf.predict_proba(X_test_ohe_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la tasa de true y false positive a medida que \n",
    "# varíamos la probabilidad \n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el áera bajo la curva\n",
    "auc = roc_auc_score(y_test, y_pred_probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, label= \"AUC = {:f}\".format(auc))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De todas formas, ya hemos visto que podemos tratar los NaNs mediante la clase `Imputer` en el módulo `preprocessing`. Veamos cómo lo aplicaríamos sobre estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mam_data.drop(columns=['bi_rads', 'severity']).values\n",
    "y = mam_data['severity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar por la media para la variable edad\n",
    "imp_age = Imputer(strategy='mean')\n",
    "X_train[:,0] = imp_age.fit_transform(X_train[:,0].reshape(-1,1)).flatten()\n",
    "X_test[:,0] = imp_age.transform(X_test[:,0].reshape(-1,1)).flatten() # Notad aquí sólo transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputar por el valor más frequente (útil para variables categóricas \n",
    "# y ordinales)\n",
    "imp_cat = Imputer(strategy='most_frequent')\n",
    "X_train[:,1:] = imp_cat.fit_transform(X_train[:,1:])\n",
    "X_test[:,1:] = imp_cat.transform(X_test[:,1:]) #Notad aquí sólo transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = oHe.fit_transform(X_train)\n",
    "X_test_ohe = oHe.transform(X_test) # Notad aquí sólo transform\n",
    "scaler = MinMaxScaler()\n",
    "X_train_ohe_scaler =  scaler.fit_transform(X_train_ohe)\n",
    "X_test_ohe_scaler =  scaler.transform(X_test_ohe) # Notad aquí sólo transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in 10**np.arange(-2,2, 0.2):\n",
    "    clf=SVC(C=C, random_state=0)\n",
    "    clf.fit(X_train_ohe_scaler, y_train)\n",
    "    plt.plot(C, clf.score(X_test_ohe_scaler, y_test), '.r') # Mejor en escala logarítmica\n",
    "    plt.semilogx(C, clf.score(X_test_ohe_scaler, y_test), '.r')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos hecho varias operaciones en este notebook: dividir en train y test, one Hot encoding + MinMaxScaler + Imputer para tratar los NaNs. Después hemos ajustado y predicho. Muchas de estas operaciones se tienen que calcular sólo sobre el training set. No podemos usar el test set para tomar decisiones sobre qué features coger, qué factor de normalización tomar para reescalar los datos, etc... Realizar tantas operaciones sobre los datos implica más probabilidad de equivocarnos. Veremos las próxima semanas que scikit tiene un módulo muy útil (ciertamente es uno de sus puntos fuertes) llamado `pipeline`, que permite encadenar muchos pasos a realizar sobre los datos en una sola línea. Esto nos llevará a economizar en ejecuciones, pero también a saber que todo lo que encadenemos se realiza en la parte de los datos adecuados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
