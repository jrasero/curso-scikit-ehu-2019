{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a ver cómo preprocesar los datos usando scikit. Cuando hacemos preprocesado de los datos, se realizan diferentes transformaciones sobre éstos, bien para eliminar o reemplazar información no útil, o bien para que los algoritmos de clasifiación funcionen correctamente. Por ejemplo, algoritmos como knn, logistic regression (con penalty) y support vector machine necesitan que los datos tengan la misma escala\n",
    "\n",
    "Scikit posee el módulo `preprocessing`, el cual contiene numerosas herramientas para llevar a cabo operaciones de transformación de datos, y el módulo `impute` (antes en `preprocessing`), para rellenar los huecos en nuestros datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/pima-indians-diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con NANs: Módulo `impute` (antes en módulo `preprocessing`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos contienen NaNs\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una opción podría ser eliminar directamente las filas que contengan NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_clean = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo malo de este approach es que podemos perder observaciones que pueden ser valiosas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se pierden muchos datos\n",
    "print(dataframe_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos intentar rellenar estos huecos reemplazando por algún valor representativo de la columna donde se encuentra el hueco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "mat_clean=imp.fit_transform(dataframe.values)\n",
    "print(mat_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando Pandas\n",
    "dataframe.fillna(dataframe.mean()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reescaleando los datos: Módulo `preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ajustar muchos algoritmos, los datos tienen que encontrarse en una misma escala para poder encontrar una solución óptima. El módulo preprocessing de scikit nos permite ocuparnos de esto de manera muy sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat_clean[:,0:8]\n",
    "y = mat_clean[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos están en una escala diversa\n",
    "print(X.mean(axis=0))\n",
    "print(X.min(axis=0))\n",
    "print(X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reescalar data (Entre 0 and 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizar data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar data\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "normalizedX = scaler.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizar los datos\n",
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=0.0)\n",
    "binaryX = binarizer.fit_transform(X)\n",
    "np.set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFECTOS DEL PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manejo de diferentes tipos de datos\n",
    "\n",
    "    Hay tres tipos de tipo de datos:\n",
    "        Numericos, e.g. income, age\n",
    "        Categóricos o nominales, e.g. gender, nationality\n",
    "        Ordinales, e.g. low/medium/high\n",
    "\n",
    "    En scikit solo features numéricas\n",
    "\n",
    "    Debemos convertir las variables categóricas y ordinales en numéricas\n",
    "        Crear dummy features\n",
    "        Transformar una feature categórica en un grupo de dummy features, cada una representando una única categoría \n",
    "        De esta manera, en un conjunto de dummy features, 1 indica que una observación pertenece a esa categoría\n",
    "\n",
    "Ejemplo extraído de :\n",
    "https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_train.csv')\n",
    "y_train_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_target_train.csv')\n",
    "X_test_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_test.csv')\n",
    "y_test_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos primero los targets a números\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_df.values)\n",
    "y_test = le.transform(y_test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miremos sólo por ahora las variables numéricas\n",
    "numerical_cols = ['ApplicantIncome', 'CoapplicantIncome','LoanAmount', \n",
    "                   'Loan_Amount_Term', 'Credit_History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN es un clasificador sensible a las diferencias entre escalas\n",
    "# (al usar distancias)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Definición del clasificador\n",
    "knn_clf=KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Ajustamos el clasificador al train\n",
    "knn_clf.fit(X_train_df.loc[:, numerical_cols].values, y_train)\n",
    "\n",
    "# Predecimos sobre el test\n",
    "print(\"accuracy = \", \n",
    "      knn_clf.score(X_test_df.loc[:, numerical_cols].values,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos lo mismo poniendo todos a la misma escala\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "# Escalamos tanto el train con el test set\n",
    "X_train_minmax=scaler.fit_transform(X_train_df.loc[:, numerical_cols].values)\n",
    "X_test_minmax=scaler.transform(X_test_df.loc[:, numerical_cols].values) # Ojo el transform aqui!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el clasificador al train\n",
    "knn_clf.fit(X_train_minmax,y_train)\n",
    "\n",
    "# Predecimos sobre el test\n",
    "print(knn_clf.score(X_test_minmax,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression también es sensible a las diferentes escalas de las \n",
    "# features\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del clasificador\n",
    "log_clf = LogisticRegression(penalty='l2', C=0.01, random_state=0)\n",
    "\n",
    "# Ajustamos el clasificador al train\n",
    "log_clf.fit(X_train_df.loc[:,numerical_vars].values, y_train)\n",
    "\n",
    "# Predecimos sobre el test\n",
    "print(\" Logistic regression antes de preprocessing: \", \n",
    "      log_clf.score(X_test_df.loc[:,numerical_cols].values, y_test))\n",
    "      \n",
    "log_clf.fit(X_train_minmax, y_train)\n",
    "print(\" Logistic regression después de preprocessing \", \n",
    "      log_clf.score(X_test_minmax,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A veces depende de cómo normalicemos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_scale=ss.fit_transform(X_train_df.loc[:,numerical_cols].values)\n",
    "\n",
    "X_test_scale=ss.transform(X_test_df.loc[:,numerical_cols].values)\n",
    "\n",
    "log_clf.fit(X_train_scale,y_train)\n",
    "print(\"Logistic Regression estandarizando: \", \n",
    "      log_clf.score(X_test_scale, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos valores son categóricos. Para poder trabajar con ellos, hace falta primero codificarlos como números y después crear un conjunto de dummy features, de tal forma que cada categoría dentro de una misma variable sea una columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por ejemplo la variable Property_Area\n",
    "property_col = X_train_df.loc[:, \"Property_Area\"].values\n",
    "print(property_col[20:30])\n",
    "\n",
    "property_col_le = le.fit_transform(property_col)\n",
    "print( property_col_le[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero claro, así muchos clasificadores supondría que entre la categoría Urban\n",
    "y Rural hay el doble de distancia que entre Urban y Semiurban. Esto no es del todo correcto. Podemos pensar en ciudades también. Por ejemplo, Bilbo, Gazteiz y Donosti. Si las codificamos como 0, 1, 2, tiene sentido que entre Bilbo y Donosti hay el doble de valor que Bilbo-Gazteiz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que crear un conjunto de dummy features, de tal forma que cada categoría dentro de una misma variable sea una columns. Esto se puede hacer con la clase `OneHotEncoder` en el módulo preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para la columna anterior, por ejemplo\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder(sparse=False)\n",
    "enc.fit_transform(property_col_le.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n",
    "          'Credit_History', 'Property_Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.loc[:, cat_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.drop(columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit_transform(X_train_df.loc[:, cat_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = np.concatenate([X_train_df.loc[:,numerical_cols].values,\n",
    "          enc.fit_transform(X_train_df.loc[:, cat_cols].values)],\n",
    "         axis=1)\n",
    "\n",
    "X_test_ohe = np.concatenate([X_test_df.loc[:,numerical_cols].values,\n",
    "          enc.transform(X_test_df.loc[:, cat_cols].values)],\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale=ss.fit_transform(X_train_ohe)\n",
    "X_test_scale=ss.transform(X_test_ohe)\n",
    "\n",
    "log_clf.fit(X_train_scale, y_train)\n",
    "\n",
    "log_clf.score(X_test_scale, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estos casos, yo recomiendo mejor usar pandas, que te permite hacer el label y one hot enconding fácilmente usando el método `get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_train.csv')\n",
    "y_train_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_target_train.csv')\n",
    "X_test_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_test.csv')\n",
    "y_test_df = pd.read_csv('https://raw.githubusercontent.com/jrasero/curso-scikit-ehu-2019/master/datasets/loan_target_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_train_df['Married'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_train_df, columns=cat_cols, drop_first=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummies = pd.concat([X_train_df.loc[:, numerical_cols],\n",
    "                             pd.get_dummies(X_train_df.loc[:, cat_cols])], \n",
    "                            axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto como la performance del clasificador puede cambiar según cómo manejemos los datos. No hay forma única y a veces es complicado saber qué procesamiento se debe adoptar. Algunos casos, como Decision Tree y Random Forest, apenas requieren mucho preprocessing. Otros, como support vector machine, logistic regression y knn requieren tratar los datos categóricos y poner todos los datos en la misma escala. Para estos casos, decidir sobre si estandarizar o sólo escalar los datos entre 0 y 1 depende de la naturaleza de los datos en si. Lo mejor, al principio es adoptar las diferentes posibilidades, comparar la performance en cada  y quedarte con el mejor de los casos\n",
    "\n",
    "Referencias:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
