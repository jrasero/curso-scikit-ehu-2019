{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a ver las diferentes maneras de seleccionar aquellas features más relevantes con respecto al target. Esto es importante ya que un exceso de features puede hacer el ajuste muy complejo y poco generalizable (overfitting). \n",
    "\n",
    "Destacan dos tipos de técnicas de feature selection: \n",
    "\n",
    "- Tipo filter, en los cuales los features se filtran según algún criterio y threshold\n",
    "- Tipo wrapper, en los cuales se usan otros métodos de clasificación para añadir o quitar features\n",
    "\n",
    "A continuación, vamos a ver cómo los diferentes tipos de feature selection están implementados en scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data['data'])\n",
    "data_df.columns = data['feature_names']\n",
    "data_df['benign'] = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos las variables según su malignidad. Esto nos permite intuir\n",
    "# cuáles son más relevantes\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, nrows=6, squeeze=False, sharex=True, figsize=(15,15))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i,col in enumerate(data['feature_names']):\n",
    "    pd.DataFrame.boxplot(data_df, column=col, by = 'benign', ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, nrows=6, \n",
    "                        squeeze=False, figsize=(15,15))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i,col in enumerate(data['feature_names']):\n",
    "    sns.violinplot(x='benign', y= col, data = data_df,  \n",
    "                   ax=axs[i], inner = 'quartile' )\n",
    "    axs[i].set_title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "feat_cors = data_df.loc[:,data['feature_names']].corr().values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "im = ax.imshow(feat_cors)\n",
    "\n",
    "ax.set_xticks(np.arange(len(data['feature_names'])))\n",
    "ax.set_yticks(np.arange(len(data['feature_names'])))\n",
    "\n",
    "\n",
    "ax.set_xticklabels(data['feature_names'])\n",
    "ax.set_yticklabels(data['feature_names'])\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(data['feature_names'])):\n",
    "    for j in range(len(data['feature_names'])):\n",
    "        text = ax.text(j, i, np.round(feat_cors[i,j],1),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "fig.tight_layout()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_df.drop(columns=['benign']).corr(), \n",
    "            center=0, square=True, cmap=\"coolwarm\",\n",
    "            xticklabels=True,yticklabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.drop(columns = ['benign']).values\n",
    "y = data_df.benign.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos primero cómo es la clasificación mediante un árbol de decisión usando todas las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=0)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "print(np.mean(cross_val_score(clf, X, y, cv=skf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que hay muchas correlaciones. Vamos a eliminar aquéllas más correlacionadas y ver cómo cambia la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['mean perimeter','mean radius','mean compactness','mean concave points',\n",
    "              'radius error','perimeter error','worst radius','worst perimeter',\n",
    "              'worst compactness','worst concave points','compactness error',\n",
    "              'concave points error','worst texture','worst area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data_df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = data_new.drop(columns = ['benign']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "print(np.mean(cross_val_score(clf, X_new, y, cv=skf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo filter: Feature selection de manera univariada\n",
    "\n",
    "En este caso,  filtramos aquellas features basado en tests univariados entre los dos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Seleccionamos las 5 mejores usando chi2\n",
    "feat_sel= SelectKBest(f_classif, k=5)\n",
    "pip = Pipeline([('feat_sel', feat_sel), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cross_val_score(pip, X, y, cv=skf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_k = []\n",
    "range_k= np.arange(1, 25,1)\n",
    "\n",
    "for k in range_k:\n",
    "    feat_sel= SelectKBest(f_classif, k=k)\n",
    "    pip = Pipeline([('feat_sel', feat_sel), ('clf', clf)])\n",
    "#    print(np.mean(cross_val_score(pip, X, y, cv=skf)))\n",
    "    res_k.append(np.mean(cross_val_score(pip, X, y, cv=skf)))\n",
    "    \n",
    "plt.plot(res_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se podría haber hecho también usando `validation_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "train_scores, test_scores = validation_curve(pip, X, y, \"feat_sel__k\",  param_range=range_k, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with Decision Tree\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.85, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(range_k, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(range_k, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(range_k, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(range_k, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo accedemos a las mejores features? Con grid search se puede hacer. Al final, feature selection es como si ajustáramos un parámetro mas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'feat_sel__k': range_k}\n",
    "\n",
    "grid = GridSearchCV(pip, param_grid,  cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "# Cuáles son estas features?\n",
    "feats_selected = grid.best_estimator_.named_steps['feat_sel'].get_support()\n",
    "print(feats_selected)\n",
    "print(data_df.drop(columns='benign').columns[feats_selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pregunta: ¿Por qué salen features correlacionadas?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo wrapper: RECURSIVE FEATURE ELIMINATION\n",
    "\n",
    "Se encuentra implementada por la clase `RFE` y `RFECV` y  \n",
    "le tenemos que pasar un clasificador que devuelva pesos o importancia de las features. Ejemplos de ellos son Logistic Regression y los árboles de decisión. Una vez calculados los pesos, va quitando features de manera recursiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator=clf, step=1, cv=skf,\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.drop(columns='benign').columns[rfecv.get_support()])\n",
    "print(np.max(rfecv.grid_scores_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo wrapper: Seleccionar de un modelo\n",
    "\n",
    "Se encuentra implementada por la clase `SelectFromModel` y otra vez, \n",
    "le pasamos un clasificador que devuelva pesos o importancia de las features. Ejemplos de ellos son Logistic Regression y los árboles de decisión. Una vez calculados los pesos, elegimos aquellas features con el peso mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([('feat_sel', SelectFromModel(DecisionTreeClassifier())),\n",
    "                ('clf', DecisionTreeClassifier())])\n",
    "print(np.mean(cross_val_score(pip, X, y, cv = skf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias: http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
