{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que hemos ido aprendiendo más y más funcionalidades de scikit, vemos que la cantidad de operaciones a realizar sobre nuestros datos aumenta. Por ejemplo, podemos tener que hacer varias operaciones de preprocessing, una feature selection, ajuste de los hiperparámetros del algoritmo y todo ello además, encerrado en un cross-validation. Para hacer todas estas operaciones de manera automática, scikit incorpora un módulo llamado `Pipeline`, que permite colocar diferentes operaciones que van a actuar de manera secuencial sobre los datos. \n",
    "\n",
    "Las ventajas de usar este módulo son:\n",
    "\n",
    "- **Convenciencia y encapsulacion**: Sólo hay que llamar una vez a `fit` y `predict` para que se realice sobre toda la secuencia de pasos\n",
    "- **Selección de parámetros unificada**: Cada paso seguramente dependa de algún hiperparámetro que ajustar. Se pueden usar los metodos *grid search* para explorar dichos parámetros a la vez\n",
    "- **Seguridad**: Evita resultados demasiado optimistas al hacer algún paso antes de cross-validation. Una vez definido el pipeline, si lo metemos dentro de un esquema de cross-validation, **todos** los pasos se realizan sobre el training set. \n",
    "\n",
    "Todas las operaciones menos la última, transforman la data. Por tanto, todos los algoritmos que implementen estos pasos han de tener un método `fit` y otro `transform`. El último puede ser de cualquier tipo, aunque lo normal es que sea el clasificador que usamos para predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a importar las tres operaciones que voy a hacer, que van a consistir en seleccionar la mejor feature mediante un anova, un reescalado de los datos,  y por último, realizar la clasificación mediante un algoritmo de tipo Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_sel = SelectKBest(k=1)\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encadenar fácilmente los tres pasos anteriores mediante la clase `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([\n",
    "    ('feat_sel', feat_sel), # Paso 1: hacer una feature selection usando ANOVA\n",
    "    ('normalizer', StandardScaler()), # Paso 2: Estandarizar los datos\n",
    "    ('clf', LogisticRegression()) # Paso 3- Clasificador\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos comprobar los pasos\n",
    "pip.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(np.mean(cross_val_score(pip, X, y, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada paso de este Pipeline puede tener algún parámetro que se pueda querer optimizar, ya que puede tener relevancia en el rendimiento final. Para ello, podemos usar un GridSearch, especificando el parámetro dentro del paso del Pipeline que estamos variando. Sólo tienes que escribir el nombre del paso en el Pipeline (el primer elemento de cada tupla), seguido por \"__\" y el parámetro a variar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'feat_sel__k': [1,2,3,4], 'clf__C': [0.01,0.1,1,10,100]}\n",
    "grid = GridSearchCV(pip, param_grid=param_grid, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder también a cada paso individual una vez ajustado todo el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_.named_steps['feat_sel'].get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver ahora cómo generaliza esto, lo podemos hacer haciéndo un holdout antes del grid search o metiéndolo dentro de un cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O usando cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cross_val_score(grid, X,y, cv = 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias: http://scikit-learn.org/stable/modules/pipeline.html#pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
