{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga las librerías de nummpy, pandas y matplotlib con el nombre de siempre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lee los datos adjuntos a este notebook, creando un dataframe de pandas con el nombre que quieras. Ajusta el argumento na_values a '?' ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_weather = pd.read_csv(filepath_or_buffer=\"data_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Redefine el dataframe anterior quitando las columnas [\"Date\",\"Location\", \"RainToday\", \"RISK_MM\"]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_weather.drop(columns=[\"Date\",\"Location\", \"RainToday\", \"RISK_MM\"], \n",
    "                 inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La data contiene huecos. Quítalos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_weather.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Las variables 'WindGustDir', 'WindDir3pm', 'WindDir9am' son categóricas. Crea dummy variables, esto es, que sean ortogonales usando un one-hot-encoding mediante la funcionalidad `get_dummies` de pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['WindGustDir', 'WindDir3pm', 'WindDir9am']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_weather_proc = pd.get_dummies(dat_weather, \n",
    "                                  columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A partir de esta dataset, define una matrix de features X y un vector de labels y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dat_weather_proc.drop(columns=['RainTomorrow']).values\n",
    "y = dat_weather_proc.loc[:, 'RainTomorrow'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Codifica el vector de labels a 0 y 1 usando `LabelEncoder`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divide el 80% de los datos para training y el 20% restante para test. Para ello usa la función `train_test_split`, fijando el random_state=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=0,\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importa un clasificador tipo arbol de decision, y crea un objeto de esta clase. Cuando definas el objeto, elige el argumento `random_state` igual a 0, `class_weight` igual a 'balanced'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0, \n",
    "                             class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ajusta el modelo sobre el train y calcula la accuracy sobre el mismo training y test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Después de ajustar el clasificador, éste da un atributo llamado \"feature_importances_\" que da un vector con la importancia de cada feature. Plotea la importancia de cada feature usando barras. Puedes usar la function bar dentro de matplotlib. Si tienes alguna duda de cómo usar la función, puedes mirar la documentación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(clf.feature_importances_)),\n",
    "        clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a calcular lo anterior para diferentes particiones de los datos. Para ello, primero importa la clase `StratifiedKFold` contenida en el módulo de model_selection. Después, define una variable que sea un objeto de esta clase y que implemente un 5-Fold cross-validation. Fija random_state=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calcula la accuracy promedio a través de estos 5 folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(X=X, y=y, estimator = clf, cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hay muchas variables, ¿verdad? Puede que no se necesiten todas. Vamos a realizar un feature selection basado en las importancias dadas por el árbol de decision anterior. El clasificador final será en este caso `Logistic Regression`. Por ello, primero importa la clase que implementa este algoritmo y define un objeto de dicho clasificador al que llamaremos \"log_clf\". Elige random_state=0 y class_weight='balanced' cuando definas este objeto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(class_weight='balanced', random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importa la clase `SelectFromModel`, que vamos a usar para seleccionar las features más importantes basado en la importancia de las features en el árbol de decision. Define un objeto de la clase recién importada y llámala \"feat\". No olvides que dicho objeto requiere que le pases el árbol de decision que va a usar para estimar la importancia de la features a seleccionar. Si tienes alguna duda, mira en la documentación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "feat = SelectFromModel(estimator=DecisionTreeClassifier(class_weight='balanced', \n",
    "                                                        random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Además, algunas features tienen escala diferente. Por ello, importa la clase `MinMaxScaler` y define un objeto a partir de esta clase, llamándola \"scaler\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a encadenar los tres objetos, que implementan diferentes operaciones, en un sólo objeto. Para ello, importa la clase `Pipeline` y crea un objeto a partir de ella llamada \"pip\". Los pasos van en este orden: primero el reescalado de los datos, después feature selection y por último el clasificador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pip = Pipeline([('scaler', scaler), \n",
    "                ('feat', feat), \n",
    "                ('clf', log_clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calcula la accuracy promediada de este objeto tipo pipeline usado el cross-validation definido más arriba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(estimator=pip, X=X, y=y, cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A veces algunos clasificadores ya implementan esto por construcción y dan un rendimiento mejor. Prueba esto con un clasificador de tipo Random Forest, implementada con el nombre `RandomForestClassifier`. Usa 50 árboles de decisión y acuérdate de elegir random_state=0 y class_weight='balanced'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(class_weight='balanced', \n",
    "                             random_state=0,\n",
    "                             n_estimators=50)\n",
    "\n",
    "cross_val_score(clf, X, y, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Extra) Repite esto último con la dataset inicial, pero rellenando los NaNs en cada columna con la mediana en sus valores observados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy = 'median')\n",
    "\n",
    "pip = Pipeline([('imputer', imp), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_weather_all = pd.read_csv(filepath_or_buffer=\"weatherAUS.csv\")\n",
    "dat_weather_all.drop(columns=[\"Date\",\"Location\", \"RainToday\", \"RISK_MM\"], \n",
    "                 inplace = True)\n",
    "dat_weather_all_proc = pd.get_dummies(dat_weather_all, \n",
    "                                      columns=categorical_columns)\n",
    "\n",
    "X_all = dat_weather_all_proc.drop(columns=['RainTomorrow']).values\n",
    "y_all = dat_weather_all_proc.loc[:, 'RainTomorrow'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pip, X_all, y_all, cv = 5).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
