{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargad las librerías de nummpy, pandas y matplotlib con el nombre de siempre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id al repositorio de UCI https://archive.ics.uci.edu/ml. Ahí, buscad la dataset con el nombre \"Heart Disease\" y entráis a su espacio. Seguidamente, id donde pone \"Download\" y entrad en Data Folder. Ahi, descargad la data con el nombre `processed.cleveland.data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leed la data descargada como si fuese un archivo csv, cargándolo como un data frame de pandas. Ajustad el argumento na_values a '?' **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data=pd.read_csv('../../datasets/processed.cleveland.data', na_values='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reescribid el nombre de las columnas a:'age', 'sex', 'cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','diagnosis'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_names = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','diagnosis']\n",
    "clev_data.columns = header_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mostrad las primeras cinco filas del data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando un método de pandas, mostrar la información sobre los tipos de las variables y el número de valores no nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando un método de pandas, mostrar la información sobre tendencia central, dispersión and tamaño de la distribución de cada variable, excluyendo NaN values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cread una nueva columna \"diagnosis_int\", tal que sea 0 si el valor en la columna \"diangosis\" es cero y 1 en caso contrario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data['diagnosis_int'] = clev_data['diagnosis'] > 0\n",
    "clev_data['diagnosis_int'] = clev_data['diagnosis_int'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminidad la columna \"diagnosis\" (No os olvidéis de asignar de nuevo el dataframe después de hacer esta operación) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data= clev_data.drop(columns=['diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Renombrar la columna \"diagnosis_int\" a \"diagnosis\" (Quizá tendréis que un método de pandas llamado \"rename\"...) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clev_data = clev_data.rename(columns={'diagnosis_int':'diagnosis'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como siempre, plotead las variables individuales diferenciando su diagnosis. Podéis usar un histograma, barplots, etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in clev_data.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(clev_data.dropna()[clev_data.dropna().loc[:,'diagnosis']==0].loc[:,var_name])\n",
    "    plt.hist(clev_data.dropna()[clev_data.dropna().loc[:,'diagnosis']==1].loc[:,var_name])\n",
    "    plt.xlabel(var_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definid, como siempre, un objeto numpy `X` que contenga los features \"age\", \"ca\" y \"thal\" y un objeto numpy `y` con el target, en este caso, la columna diagnosis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clev_data[['age', 'ca','thal']].values\n",
    "y = clev_data['diagnosis'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La variable \"ca\" tiene NaNs. Usad la clase `Imputer` en scikit para reemplazar los NaN por el valor más frecuente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp= Imputer(strategy='most_frequent')\n",
    "X = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A su vez, la variable \"ca\" y \"thal\" son variables categóricas. Usad la clase `OneHotEncoder` en el módulo preprocessing de scikit para codificar estas categorías. Mirad cómo aumenta el tamaño de los datos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oHe = OneHotEncoder(categorical_features=[1,2],sparse=False)\n",
    "X= oHe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cread una partición de los datos train y test, donde el primero tenga el 70% de los datos y el segundo el 30% restante. Podéis usar la función `train_test_split` de scikit usando un random_state=0** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculad la performance del clasificador usando un árbol de decisión y random forest. En ambos casos, fijad el valor de random_state=0 en clasificador y mostrad los resultados variando el hiperparámetro max_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in range(1,10):\n",
    "    clf = DecisionTreeClassifier(random_state=0, max_depth=depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"acc con depth \", depth, \" es igual a= \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"acc con depth \", depth, \" es igual a= \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for depth in range(1,10):\n",
    "    clf = RandomForestClassifier(random_state=0, max_depth=depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"acc con depth \", depth, \" es igual a= \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otros clasificadores necesitan que las variables tengan la misma escala. Volved a hacer todo el preprocesing anterior sobre X ( Imputer y one hot encoder), añadiendo como último paso la aplicación de la clase `MinMaxScaler` en el módulo de preprocessing de scikit, que permite reescalar los datos entre 0 y 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Volved a crear una partición de los datos train y test, donde el primero tenga el 70% de los datos y el segundo el 30% restante. Podéis usar la función `train_test_split` de scikit usando un random_state=0** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculad la performance del clasificador usando logistic regression, support vector machines y k-nearest neighbors. En todos los casos, mostrad los resultados variando los hiperparámetros de los algoritmos (por ejemplo, C en los dos primeros y n_neighbors en el último)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
